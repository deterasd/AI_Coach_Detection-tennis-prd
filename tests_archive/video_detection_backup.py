import cv2
import numpy as np
from ultralytics import YOLO
import time
import torch
import gc
import os

# COCO é è¨­ 17 å€‹é—œç¯€çš„åç¨±ï¼Œå¯è¦–éœ€æ±‚èª¿æ•´/å¢åŠ 
body_parts_list = [
    "nose", "left_eye", "right_eye", "left_ear", "right_ear",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
    "left_wrist", "right_wrist", "left_hip", "right_hip",
    "left_knee", "right_knee", "left_ankle", "right_ankle"
]

def resize_frame(frame, width=None, height=None, inter=cv2.INTER_AREA):
    if width is None and height is None:
        return frame
    h, w = frame.shape[:2]
    if width is None:
        r = height / float(h)
        dim = (int(w * r), height)
    else:
        r = width / float(w)
        dim = (width, int(h * r))
    return cv2.resize(frame, dim, interpolation=inter)

def process_video(
    video_path,
    ball_model_path='model/tennisball_OD_v1.pt',
    pose_model_path='model/yolov8n-pose.pt',
    OUTPUT_WIDTH=1280,
    OUTPUT_HEIGHT=720,
    skip_frames=1,
    yolo_batch_size=4,
    ball_conf_threshold=0.8
):
    start_time = time.time()
    print(f"ğŸ“¹ é–‹å§‹è™•ç†å½±ç‰‡: {video_path}")
    print(f"   åƒæ•¸: å¯¬åº¦={OUTPUT_WIDTH}, é«˜åº¦={OUTPUT_HEIGHT}, æ‰¹æ¬¡å¤§å°={yolo_batch_size}")
    
    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"   ä½¿ç”¨è¨­å‚™: {device_str}")

    try:
        # åªåœ¨æ­¤è™•è¼‰å…¥æ¨¡å‹ä¸€æ¬¡ï¼Œä¸¦ç§»è‡³æŒ‡å®šè£ç½®
        print("ğŸ¤– è¼‰å…¥AIæ¨¡å‹...")
        ball_model = YOLO(ball_model_path)
        pose_model = YOLO(pose_model_path)
        ball_model.model.to(device_str)
        pose_model.model.to(device_str)
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")

        print("ğŸ“– è®€å–å½±ç‰‡å¹€...")
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ ç„¡æ³•è®€å–å½±ç‰‡: {video_path}")
            return None

        original_fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        print(f"   å½±ç‰‡è³‡è¨Š: {total_video_frames} å¹€, {original_fps} FPS")

        frames_for_output = []
        frames_for_infer = []
        infer_indices = []

        frame_idx = 0
        read_start = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame_idx += 1

            try:
                resized_frame = resize_frame(frame, OUTPUT_WIDTH, OUTPUT_HEIGHT)
                frames_for_output.append(resized_frame)

                if frame_idx % skip_frames == 0:
                    frames_for_infer.append(resized_frame)
                    infer_indices.append(frame_idx)
            except Exception as e:
                print(f"âš ï¸ è™•ç†ç¬¬ {frame_idx} å¹€å¤±æ•—: {e}")
                continue
            
            # é€²åº¦æ›´æ–°
            if frame_idx % 100 == 0:
                elapsed = time.time() - read_start
                print(f"   è®€å–é€²åº¦: {frame_idx}/{total_video_frames} å¹€ ({elapsed:.1f}s)")

        cap.release()
        
        total_frames = len(frames_for_output)
        if total_frames == 0:
            print("âŒ æ²’æœ‰æˆåŠŸè®€å–ä»»ä½•å½±ç‰‡å¹€")
            return None
        
        print(f"âœ… å½±ç‰‡è®€å–å®Œæˆ: {total_frames} å¹€ ({len(frames_for_infer)} å¹€ç”¨æ–¼æ¨ç†)")

        # AIæ¨ç†
        print("ğŸ§  é–‹å§‹AIæ¨ç†...")
        inference_start = time.time()
        
        # æ¨è«–æ™‚åŠ å…¥ no_grad ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨
        with torch.no_grad():
            print(f"   å§¿æ…‹æª¢æ¸¬: {len(frames_for_infer)} å¹€")
            pose_results_batch = pose_model.predict(
                frames_for_infer,
                verbose=False,
                device=device_str,
                batch=yolo_batch_size
            )
            
            print(f"   ç¶²çƒæª¢æ¸¬: {len(frames_for_infer)} å¹€")
            ball_results_batch = ball_model.predict(
                frames_for_infer,
                verbose=False,
                device=device_str,
                batch=yolo_batch_size
            )
        
        inference_time = time.time() - inference_start
        print(f"âœ… AIæ¨ç†å®Œæˆ ({inference_time:.2f}s)")

        # è™•ç†çµæœ
        print("ğŸ“Š è™•ç†AIçµæœ...")
        ball_positions = [None] * total_frames
        ball_confidences = [None] * total_frames
        keypoints_per_frame = [None] * total_frames

        for i, fidx in enumerate(infer_indices):
            try:
                pose_result = pose_results_batch[i]
                ball_result = ball_results_batch[i]

                # è™•ç†å§¿æ…‹æª¢æ¸¬çµæœ
                if (pose_result.keypoints is not None and 
                    len(pose_result.keypoints) > 0 and 
                    len(pose_result.keypoints.xy) > 0):
                    kpts = pose_result.keypoints.xy[0]  # shape (17,2)
                    kpts_xy = [(int(x), int(y)) for x, y in kpts]
                else:
                    kpts_xy = None

                # è™•ç†ç¶²çƒæª¢æ¸¬çµæœ
                boxes = ball_result.boxes
                if boxes is not None and len(boxes) > 0:
                    box = boxes[0]
                    if float(box.conf[0]) >= ball_conf_threshold:
                        x1, y1, x2, y2 = box.xyxy[0]
                        cx = int((x1 + x2) / 2)
                        cy = int((y1 + y2) / 2)
                        ball_pos = (cx, cy)
                        ball_conf = float(box.conf[0])
                    else:
                        ball_pos = None
                        ball_conf = None
                else:
                    ball_pos = None
                    ball_conf = None

                idx_in_list = fidx - 1
                ball_positions[idx_in_list] = ball_pos
                ball_confidences[idx_in_list] = ball_conf
                keypoints_per_frame[idx_in_list] = kpts_xy
                
            except Exception as e:
                print(f"âš ï¸ è™•ç†ç¬¬ {i} å€‹æ¨ç†çµæœå¤±æ•—: {e}")
                continue

        print("ğŸ”„ è£œå…¨ç¼ºå¤±æ•¸æ“š...")
        # è£œå…¨ç¼ºå¤±è³‡æ–™ï¼šè‹¥ç•¶å‰å¹€è³‡æ–™ç¼ºå¤±å‰‡ä½¿ç”¨å‰ä¸€å¹€è£œä¸Š
        last_ball = None
        last_conf = None
        last_kpts = None
        for i in range(total_frames):
            if ball_positions[i] is None:
                ball_positions[i] = last_ball
                ball_confidences[i] = last_conf
            else:
                last_ball = ball_positions[i]
                last_conf = ball_confidences[i]

            if keypoints_per_frame[i] is None:
                keypoints_per_frame[i] = last_kpts
            else:
                last_kpts = keypoints_per_frame[i]

        print("ğŸ¥ ç”Ÿæˆè¼¸å‡ºå½±ç‰‡...")
        output_path = video_path.replace('.mp4', '_processed.mp4')
        info_panel_width = 400
        output_width = OUTPUT_WIDTH + info_panel_width
        output_height = OUTPUT_HEIGHT

        # å˜—è©¦å‰µå»ºVideoWriter
        video_writer_created = False
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, original_fps, (output_width, output_height))
        
        # æª¢æŸ¥ VideoWriter æ˜¯å¦æˆåŠŸåˆå§‹åŒ–
        if not out.isOpened():
            print(f"âŒ VideoWriter åˆå§‹åŒ–å¤±æ•—: {output_path}")
            print(f"ğŸ”§ å˜—è©¦ä½¿ç”¨ä¸åŒçš„ç·¨ç¢¼å™¨...")
            
            # å˜—è©¦å…¶ä»–ç·¨ç¢¼å™¨
            codecs_to_try = [
                ('XVID', cv2.VideoWriter_fourcc(*'XVID')),
                ('H264', cv2.VideoWriter_fourcc(*'H264')),
                ('X264', cv2.VideoWriter_fourcc(*'X264')),
                ('MP4V', cv2.VideoWriter_fourcc(*'MP4V')),
            ]
            
            for codec_name, codec in codecs_to_try:
                print(f"ğŸ”§ å˜—è©¦ {codec_name} ç·¨ç¢¼å™¨...")
                out = cv2.VideoWriter(output_path, codec, original_fps, (output_width, output_height))
                if out.isOpened():
                    print(f"âœ… {codec_name} ç·¨ç¢¼å™¨æˆåŠŸ")
                    video_writer_created = True
                    break
                else:
                    out.release()
            
            # å¦‚æœæ‰€æœ‰ç·¨ç¢¼å™¨éƒ½å¤±æ•—ï¼Œå˜—è©¦ AVI æ ¼å¼
            if not video_writer_created:
                print("ğŸ”§ å˜—è©¦ AVI æ ¼å¼...")
                output_path = output_path.replace('.mp4', '.avi')
                out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), original_fps, (output_width, output_height))
                
                if out.isOpened():
                    video_writer_created = True
                    print("âœ… AVI æ ¼å¼æˆåŠŸ")
                else:
                    print("âŒ æ‰€æœ‰å½±ç‰‡ç·¨ç¢¼å™¨éƒ½å¤±æ•—ï¼Œè¿”å›åŸå§‹æª”æ¡ˆ")
                    return video_path
        else:
            video_writer_created = True
            print("âœ… MP4V ç·¨ç¢¼å™¨æˆåŠŸ")

        if not video_writer_created:
            print("âŒ ç„¡æ³•å‰µå»ºå½±ç‰‡å¯«å…¥å™¨")
            return None

        TRACKED_KEYPOINTS = [10]
        keypoint_trails = {kp: [] for kp in TRACKED_KEYPOINTS}
        ball_trail = []

        print("âœï¸ å¯«å…¥å½±ç‰‡å¹€...")
        write_start = time.time()
        
        for i in range(total_frames):
            try:
                frame = frames_for_output[i].copy()
                ball_pos = ball_positions[i]
                ball_conf = ball_confidences[i]
                kpts = keypoints_per_frame[i]

                ball_trail.append(ball_pos)
                if kpts is not None:
                    for kp_idx in TRACKED_KEYPOINTS:
                        if kp_idx < len(kpts):
                            keypoint_trails[kp_idx].append(kpts[kp_idx])
                        else:
                            keypoint_trails[kp_idx].append(None)
                else:
                    for kp_idx in TRACKED_KEYPOINTS:
                        keypoint_trails[kp_idx].append(None)

                # è¦–è¦ºåŒ–è™•ç†ï¼ˆç°¡åŒ–ç‰ˆï¼‰
                combined_frame = np.zeros((output_height, output_width, 3), dtype=np.uint8)
                combined_frame[:, :OUTPUT_WIDTH] = frame
                
                # æ·»åŠ åŸºæœ¬è³‡è¨Šåˆ°å³å´é¢æ¿
                info_text = f"Frame: {i+1}/{total_frames}"
                cv2.putText(combined_frame, info_text, (OUTPUT_WIDTH + 10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
                
                if ball_pos:
                    ball_text = f"Ball: ({ball_pos[0]}, {ball_pos[1]})"
                    cv2.putText(combined_frame, ball_text, (OUTPUT_WIDTH + 10, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
                    cv2.circle(frame, ball_pos, 8, (0, 255, 255), -1)
                
                # ç¹ªè£½é—œéµé»
                if kpts is not None:
                    for j, (x, y) in enumerate(kpts):
                        if x > 0 and y > 0:
                            cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)
                
                # æ›´æ–°åˆä½µå¹€
                combined_frame[:, :OUTPUT_WIDTH] = frame
                
                # å¯«å…¥å½±ç‰‡å¹€
                out.write(combined_frame)
                
                # é€²åº¦æ›´æ–°
                if (i + 1) % 100 == 0:
                    elapsed = time.time() - write_start
                    progress = (i + 1) / total_frames * 100
                    print(f"   å¯«å…¥é€²åº¦: {progress:.1f}% ({i+1}/{total_frames}) - {elapsed:.1f}s")
                    
            except Exception as e:
                print(f"âš ï¸ è™•ç†ç¬¬ {i} å¹€å¤±æ•—: {e}")
                # å¯«å…¥é»‘è‰²å¹€ä½œç‚ºæ›¿ä»£
                black_frame = np.zeros((output_height, output_width, 3), dtype=np.uint8)
                out.write(black_frame)
                continue

        out.release()
        write_time = time.time() - write_start
        print(f"âœ… å½±ç‰‡å¯«å…¥å®Œæˆ ({write_time:.2f}s)")
        
        # æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆ
        if os.path.exists(output_path):
            file_size = os.path.getsize(output_path)
            if file_size < 1000:  # æª”æ¡ˆå°æ–¼ 1KB è¡¨ç¤ºå¯èƒ½æœ‰å•é¡Œ
                print(f"âš ï¸ è¼¸å‡ºæª”æ¡ˆå¯èƒ½æœ‰å•é¡Œ: {output_path} (å¤§å°: {file_size} bytes)")
            else:
                print(f"âœ… å½±ç‰‡è™•ç†å®Œæˆ: {output_path} (å¤§å°: {file_size} bytes)")
        else:
            print(f"âŒ è¼¸å‡ºæª”æ¡ˆæœªå‰µå»º: {output_path}")
            return None

        # æ¸…ç†è¨˜æ†¶é«”
        print("ğŸ§¹ æ¸…ç†è¨˜æ†¶é«”...")
        del frames_for_output, frames_for_infer, ball_positions, ball_confidences, keypoints_per_frame
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        total_time = time.time() - start_time
        print(f"ğŸ‰ å½±ç‰‡è™•ç†ç¸½è€—æ™‚: {total_time:.2f}ç§’")
        
        return output_path
        
    except Exception as e:
        print(f"âŒ å½±ç‰‡è™•ç†ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # æ¸…ç†è¨˜æ†¶é«”
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return None
        else:
            for kp_idx in TRACKED_KEYPOINTS:
                keypoint_trails[kp_idx].append(None)

        valid_ball_positions = [p for p in ball_trail if p is not None]
        for b in range(1, len(valid_ball_positions)):
            p1 = valid_ball_positions[b - 1]
            p2 = valid_ball_positions[b]
            if p1 and p2:
                progress = b / len(valid_ball_positions)
                color = (0, int(255*(1 - progress)), int(255*progress))
                cv2.line(frame, p1, p2, color, 4)

        for kp_idx, trail in keypoint_trails.items():
            valid_trail = [p for p in trail if p is not None]
            for t in range(1, len(valid_trail)):
                p1 = valid_trail[t-1]
                p2 = valid_trail[t]
                progress = t / len(valid_trail)
                color = (int(255*(1 - progress)), int(255*progress), 0)
                cv2.line(frame, p1, p2, color, 4)

        if kpts is not None:
            for idx, (xx, yy) in enumerate(kpts):
                color = (0, 0, 255) if idx in TRACKED_KEYPOINTS else (0, 255, 0)
                cv2.circle(frame, (xx, yy), 5, color, -1)
                cv2.putText(frame, str(idx), (xx+5, yy+10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)

        info_panel = np.ones((output_height, info_panel_width, 3), dtype=np.uint8) * 40
        header_height = 50
        cv2.rectangle(info_panel, (0, 0), (info_panel_width, header_height), (0, 150, 0), -1)
        cv2.putText(info_panel, "Tennis Ball Detection", (10, 35),
                    cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 2)

        y_text = header_height + 30
        if ball_pos is not None:
            cv2.putText(info_panel, "Ball Status: Detected", (10, y_text),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        else:
            cv2.putText(info_panel, "Ball Status: Not Detected", (10, y_text),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        y_text += 30

        if ball_pos is not None:
            cx, cy = ball_pos
            cv2.putText(info_panel, f"Position: ({cx}, {cy})", (10, y_text),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 1)
            y_text += 30
            if ball_conf is not None:
                cv2.putText(info_panel, f"Confidence: {ball_conf:.2f}", (10, y_text),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 1)
                y_text += 30

        pose_header_height = 40
        pose_header_top = y_text
        pose_header_bottom = pose_header_top + pose_header_height
        cv2.rectangle(info_panel,
                      (0, pose_header_top),
                      (info_panel_width, pose_header_bottom),
                      (255, 100, 0),
                      -1)
        cv2.putText(info_panel, "Pose Estimation",
                    (10, pose_header_top + 28),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8,
                    (255, 255, 255), 2)

        y_text = pose_header_bottom + 30
        if kpts is not None:
            for idx, part_name in enumerate(body_parts_list):
                if idx < len(kpts):
                    xx, yy = kpts[idx]
                    text_line = f"{part_name:<15}: ({xx}, {yy})"
                else:
                    text_line = f"{part_name:<15}: ( -, - )"
                cv2.putText(info_panel, text_line, (10, y_text),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (220,220,220), 1)
                y_text += 22
                if y_text >= output_height - 10:
                    break
        else:
            cv2.putText(info_panel, "No keypoints found",
                        (10, y_text),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            
        if frame.shape[0] != output_height:
            frame = cv2.resize(frame, (OUTPUT_WIDTH, output_height))
        
        # ç¢ºä¿å½±ç‰‡å¹€ç¶­åº¦æ­£ç¢º
        if frame.shape != (output_height, OUTPUT_WIDTH, 3):
            frame = cv2.resize(frame, (OUTPUT_WIDTH, output_height))
        
        combined_frame = np.hstack((frame, info_panel))
        
        # æª¢æŸ¥åˆä½µå¾Œçš„å¹€ç¶­åº¦
        if combined_frame.shape != (output_height, output_width, 3):
            print(f"âš ï¸ å¹€ {i}: ç¶­åº¦ä¸åŒ¹é… {combined_frame.shape} != ({output_height}, {output_width}, 3)")
            combined_frame = cv2.resize(combined_frame, (output_width, output_height))
        
        # å¯«å…¥å½±ç‰‡å¹€
        out.write(combined_frame)
        # æ³¨æ„ï¼šOpenCV çš„ write() è¿”å›å€¼ä¸å¯é ï¼Œä¸æª¢æŸ¥è¿”å›å€¼

    out.release()
    
    # æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆæ˜¯å¦æˆåŠŸå‰µå»º
    if os.path.exists(output_path):
        file_size = os.path.getsize(output_path)
        if file_size < 1000:  # æª”æ¡ˆå°æ–¼ 1KB è¡¨ç¤ºå¯èƒ½æœ‰å•é¡Œ
            print(f"âš ï¸ è¼¸å‡ºæª”æ¡ˆå¯èƒ½æœ‰å•é¡Œ: {output_path} (å¤§å°: {file_size} bytes)")
        else:
            print(f"âœ… å½±ç‰‡è™•ç†å®Œæˆ: {output_path} (å¤§å°: {file_size} bytes)")
    else:
        print(f"âŒ è¼¸å‡ºæª”æ¡ˆæœªå‰µå»º: {output_path}")

    # æ¸…ç†ä¸­é–“è³‡æ–™ï¼Œé¿å…ç´¯ç©
    del frames_for_output, frames_for_infer, ball_positions, ball_confidences, keypoints_per_frame
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return output_path

if __name__ == "__main__":
    total_start = time.time()
    video_path = 'æ¸¬è©¦2__1_45_compressed.mp4'
    output_path = process_video(video_path)
    total_end = time.time()
    print(f"===== ç¨‹å¼ç¸½è€—æ™‚: {total_end - total_start:.2f} ç§’ =====")
