"""
Step 1: é‡æŠ•å½±èª¤å·®é©—è­‰åˆ†æ
é©—è­‰ 3D é‡å»ºçµæœçš„é‡æŠ•å½±ç²¾åº¦ï¼ŒåŒ…å«ç•¸è®Šåƒæ•¸æª¢æŸ¥å’Œç›¸æ©Ÿå…§åƒé©—è­‰

åŠŸèƒ½ï¼š
  1. 2D/3D é‡æŠ•å½±èª¤å·®è¨ˆç®—
  2. å„é—œç¯€é»èª¤å·®çµ±è¨ˆ
  3. ç•°å¸¸å€¼æª¢æ¸¬
  4. é€å¹€èª¤å·®åˆ†æ
  5. èª¤å·® vs æ·±åº¦é—œä¿‚åˆ†æ
  6. æ™‚é–“ç©©å®šæ€§åˆ†æ
  7. ç›¸æ©Ÿåå·®åˆ†æ
  8. ç•¸è®Šåƒæ•¸é©—è­‰
  9. ç›¸æ©Ÿå…§åƒåˆç†æ€§æª¢æŸ¥
"""

import numpy as np
from datetime import datetime
import sys
from typing import Optional, Tuple

# å¼•å…¥å…±ç”¨æ¨¡çµ„
from .utils import (
    get_keypoint_safely,
    get_keypoint_2d,
    is_valid_keypoint,
    load_json_file,
    validate_frame_structure,
    convert_to_serializable,
    save_json_results,
    generate_output_path,
    KEYPOINT_NAMES_EN,
)
from config import load_config, ValidationConfig


# ========================================================
# æŠ•å½±ç›¸é—œå‡½æ•¸
# ========================================================

def project_point(P: np.ndarray, X: np.ndarray, distortion: np.ndarray = None) -> np.ndarray:
    """
    ä½¿ç”¨æŠ•å½±çŸ©é™£ P å°‡ 3D é½Šæ¬¡åº§æ¨™æŠ•å½±åˆ° 2D
    
    åƒæ•¸:
        P: æŠ•å½±çŸ©é™£ (3x4)
        X: 3D é½Šæ¬¡åº§æ¨™ (4,)
        distortion: ç•¸è®Šåƒæ•¸ [k1, k2, p1, p2, k3] (å¯é¸)
    
    è¿”å›:
        np.ndarray: 2D æŠ•å½±åº§æ¨™ [x, y]ï¼Œè‹¥æŠ•å½±å¤±æ•—å‰‡è¿”å› [nan, nan]
    """
    x = P @ X
    if abs(x[2]) < 1e-6:
        return np.array([np.nan, np.nan])
    
    # æ­¸ä¸€åŒ–æŠ•å½±åº§æ¨™
    x_norm = x[:2] / x[2]
    
    # è‹¥æœ‰ç•¸è®Šåƒæ•¸ï¼Œé€²è¡Œç•¸è®Šæ ¡æ­£
    if distortion is not None:
        x_norm = apply_distortion(x_norm, distortion)
    
    return x_norm


def apply_distortion(point: np.ndarray, distortion: np.ndarray) -> np.ndarray:
    """
    æ‡‰ç”¨é¡é ­ç•¸è®Šæ¨¡å‹
    
    åƒæ•¸:
        point: æ­¸ä¸€åŒ–åº§æ¨™ [x, y]
        distortion: ç•¸è®Šåƒæ•¸ [k1, k2, p1, p2, k3]
    
    è¿”å›:
        np.ndarray: ç•¸è®Šå¾Œçš„åº§æ¨™
    """
    if len(distortion) < 5:
        return point
    
    k1, k2, p1, p2, k3 = distortion[:5]
    x, y = point[0], point[1]
    r2 = x**2 + y**2
    
    # å¾‘å‘ç•¸è®Š
    radial = 1 + k1*r2 + k2*r2**2 + k3*r2**3
    
    # åˆ‡å‘ç•¸è®Š
    x_distorted = x * radial + 2*p1*x*y + p2*(r2 + 2*x**2)
    y_distorted = y * radial + p1*(r2 + 2*y**2) + 2*p2*x*y
    
    return np.array([x_distorted, y_distorted])


def rq_decomposition(matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """åŸ·è¡Œ 3x3 çŸ©é™£çš„ RQ åˆ†è§£ä»¥å–å¾— K èˆ‡ Rã€‚"""
    if matrix.shape != (3, 3):
        raise ValueError("RQ åˆ†è§£åƒ…æ”¯æ´ 3x3 çŸ©é™£")

    m = matrix.astype(float)
    # åˆ©ç”¨ QR åˆ†è§£æ¨å° RQï¼Œé¿å…ä¾è³´é¡å¤–å¥—ä»¶
    q, r = np.linalg.qr(np.flipud(m).T)
    r = np.flipud(r.T)
    q = q.T
    r = np.fliplr(r)
    q = np.flipud(q)

    diag = np.sign(np.diag(r))
    diag[diag == 0] = 1
    d = np.diag(diag)
    r = r @ d
    q = d @ q

    return r, q


def extract_intrinsics(P: np.ndarray) -> Optional[np.ndarray]:
    """å¾æŠ•å½±çŸ©é™£ä¸­æå–å…§åƒ Kï¼Œè‹¥å¤±æ•—å‰‡è¿”å› Noneã€‚"""
    try:
        M = P[:, :3]
        if np.linalg.matrix_rank(M) < 3:
            return None
        K, _ = rq_decomposition(M)
        if abs(K[2, 2]) > 1e-8:
            K = K / K[2, 2]
        return K
    except Exception:
        return None


def safe_ratio(numerator: float, denominator: float) -> float:
    """é¿å…é›¶é™¤çš„æ¯”ä¾‹è¨ˆç®—ã€‚"""
    return float(numerator / denominator * 100) if denominator > 0 else 0.0


def analyze_error_depth_relationship(error_records: list) -> Optional[dict]:
    """æ ¹æ“šèª¤å·®ç´€éŒ„åˆ†æèª¤å·®èˆ‡æ·±åº¦çš„é—œè¯ã€‚"""
    if not error_records:
        return None

    depths = np.array([rec[4] for rec in error_records], dtype=float)
    errors = np.array([rec[3] for rec in error_records], dtype=float)

    if len(depths) < 3 or np.std(depths) < 1e-6:
        return None

    corr = float(np.corrcoef(depths, errors)[0, 1])
    slope, intercept = np.polyfit(depths, errors, 1)

    return {
        "corr": corr,
        "slope": float(slope),
        "intercept": float(intercept),
        "depth_min": float(depths.min()),
        "depth_max": float(depths.max())
    }


def analyze_temporal_stability(per_frame_side: np.ndarray, per_frame_45: np.ndarray, config: ValidationConfig) -> dict:
    """åˆ†æé€å¹€å¹³å‡èª¤å·®çš„ç©©å®šæ€§ã€‚"""
    def _analyze(series: np.ndarray) -> dict:
        valid = series[~np.isnan(series)]
        if valid.size == 0:
            return {"samples": 0}

        diffs = np.abs(np.diff(valid))
        spike_threshold = max(config.reprojection_good * 0.5, 1.0)
        spikes = int(np.sum(diffs > spike_threshold))

        return {
            "samples": int(valid.size),
            "mean": float(np.mean(valid)),
            "std": float(np.std(valid)),
            "cv": float(np.std(valid) / np.mean(valid) * 100) if np.mean(valid) > 1e-6 else 0.0,
            "spike_count": spikes,
            "max_jump": float(np.max(diffs)) if diffs.size else 0.0
        }

    return {
        "side": _analyze(per_frame_side),
        "camera_45": _analyze(per_frame_45)
    }


def analyze_detailed_anomalies(error_records: list, config: ValidationConfig) -> dict:
    """
    è©³ç´°åˆ†æç•°å¸¸èª¤å·®çš„åˆ†ç´šã€åˆ†å¸ƒå’Œæ¨¡å¼
    
    åƒæ•¸:
        error_records: èª¤å·®è¨˜éŒ„åˆ—è¡¨ [(camera, frame, keypoint, error, depth), ...]
        config: é©—è­‰é…ç½®
    
    è¿”å›:
        dict: è©³ç´°ç•°å¸¸åˆ†æçµæœ
    """
    if not error_records:
        return {}
    
    # ç¯©é¸ç•°å¸¸å€¼
    threshold = config.reprojection_outlier_threshold
    anomalies = [rec for rec in error_records if rec[3] > threshold]
    
    if not anomalies:
        return {
            "total_count": 0,
            "by_severity": {"severe": [], "moderate": [], "mild": []},
            "by_keypoint": {},
            "continuous_segments": []
        }
    
    # åš´é‡ç¨‹åº¦åˆ†ç´š
    severe_threshold = threshold * 2  # >40px
    moderate_threshold = threshold * 1.5  # 30-40px
    
    severe = [a for a in anomalies if a[3] > severe_threshold]
    moderate = [a for a in anomalies if moderate_threshold < a[3] <= severe_threshold]
    mild = [a for a in anomalies if threshold < a[3] <= moderate_threshold]
    
    # æŒ‰é—œç¯€é»çµ±è¨ˆ
    keypoint_stats = {}
    for cam, frame, kp, err, depth in anomalies:
        if kp not in keypoint_stats:
            keypoint_stats[kp] = {"side": 0, "45": 0, "errors": [], "frames": []}
        keypoint_stats[kp]["side" if cam == "side" else "45"] += 1
        keypoint_stats[kp]["errors"].append(float(err))
        keypoint_stats[kp]["frames"].append(int(frame))
    
    # è¨ˆç®—æ¯å€‹é—œç¯€é»çš„çµ±è¨ˆ
    for kp, stats in keypoint_stats.items():
        stats["count"] = len(stats["errors"])
        stats["mean_error"] = float(np.mean(stats["errors"]))
        stats["max_error"] = float(np.max(stats["errors"]))
        stats["side_ratio"] = stats["side"] / stats["count"] if stats["count"] > 0 else 0
    
    # æª¢æ¸¬é€£çºŒç•°å¸¸å€æ®µ
    continuous_segments = []
    anomalies_sorted = sorted(anomalies, key=lambda x: (x[2], x[0], x[1]))  # æŒ‰ keypoint, camera, frame
    
    current_segment = None
    for cam, frame, kp, err, depth in anomalies_sorted:
        if current_segment is None:
            current_segment = {
                "keypoint": kp,
                "camera": cam,
                "start_frame": frame,
                "end_frame": frame,
                "errors": [err]
            }
        elif (current_segment["keypoint"] == kp and 
              current_segment["camera"] == cam and 
              frame - current_segment["end_frame"] <= 2):  # å…è¨± 1-2 å¹€é–“éš”
            current_segment["end_frame"] = frame
            current_segment["errors"].append(err)
        else:
            if len(current_segment["errors"]) >= 5:  # è‡³å°‘é€£çºŒ 5 å¹€
                current_segment["duration"] = current_segment["end_frame"] - current_segment["start_frame"] + 1
                current_segment["mean_error"] = float(np.mean(current_segment["errors"]))
                current_segment["max_error"] = float(np.max(current_segment["errors"]))
                continuous_segments.append(current_segment)
            current_segment = {
                "keypoint": kp,
                "camera": cam,
                "start_frame": frame,
                "end_frame": frame,
                "errors": [err]
            }
    
    # æª¢æŸ¥æœ€å¾Œä¸€å€‹å€æ®µ
    if current_segment and len(current_segment["errors"]) >= 5:
        current_segment["duration"] = current_segment["end_frame"] - current_segment["start_frame"] + 1
        current_segment["mean_error"] = float(np.mean(current_segment["errors"]))
        current_segment["max_error"] = float(np.max(current_segment["errors"]))
        continuous_segments.append(current_segment)
    
    # ç§»é™¤ errors åˆ—è¡¨ï¼ˆå¤ªå¤§ï¼‰
    for seg in continuous_segments:
        del seg["errors"]
    
    return {
        "total_count": len(anomalies),
        "by_severity": {
            "severe": severe,
            "moderate": moderate,
            "mild": mild
        },
        "by_keypoint": keypoint_stats,
        "continuous_segments": sorted(continuous_segments, key=lambda x: x["duration"], reverse=True)
    }


def validate_camera_intrinsics(P: np.ndarray, camera_name: str = "Camera") -> dict:
    """
    é©—è­‰ç›¸æ©Ÿå…§åƒçš„åˆç†æ€§
    
    åƒæ•¸:
        P: æŠ•å½±çŸ©é™£ (3x4)
        camera_name: ç›¸æ©Ÿåç¨±
    
    è¿”å›:
        dict: é©—è­‰çµæœ
    """
    results = {
        "camera_name": camera_name,
        "is_valid": True,
        "warnings": [],
        "errors": []
    }
    
    # æå–å…§åƒçŸ©é™£ K
    K = extract_intrinsics(P)
    if K is None:
        results["errors"].append("æŠ•å½±çŸ©é™£ç„¡æ³•åˆ†è§£å‡ºæœ‰æ•ˆå…§åƒï¼Œè«‹æª¢æŸ¥è¼¸å…¥ P")
        results["is_valid"] = False
        return results
    
    # æª¢æŸ¥ç„¦è· (fx, fy)
    fx = K[0, 0]
    fy = K[1, 1]
    
    if fx <= 0 or fy <= 0:
        results["errors"].append(f"ç„¦è·ç•°å¸¸: fx={fx:.2f}, fy={fy:.2f}")
        results["is_valid"] = False
    
    # æª¢æŸ¥ç„¦è·æ¯”ä¾‹ï¼ˆé€šå¸¸æ¥è¿‘ 1ï¼‰
    if abs(fx / fy - 1.0) > 0.1:
        results["warnings"].append(f"ç„¦è·æ¯”ä¾‹ç•°å¸¸: fx/fy={fx/fy:.3f}")
    
    # æª¢æŸ¥ä¸»é» (cx, cy)
    cx = K[0, 2]
    cy = K[1, 2]
    
    if cx < 0 or cy < 0:
        results["warnings"].append(f"ä¸»é»åº§æ¨™ç•°å¸¸: cx={cx:.2f}, cy={cy:.2f}")
    
    # æª¢æŸ¥éå°è§’å…ƒç´ ï¼ˆå‚¾æ–œåƒæ•¸ï¼Œé€šå¸¸æ¥è¿‘ 0ï¼‰
    skew = K[0, 1]
    if abs(skew) > 10:
        results["warnings"].append(f"å‚¾æ–œåƒæ•¸ç•°å¸¸: skew={skew:.2f}")
    
    results["intrinsics"] = {
        "fx": float(fx),
        "fy": float(fy),
        "cx": float(cx),
        "cy": float(cy),
        "skew": float(skew),
        "aspect_ratio": float(fx / fy) if fy > 0 else 0.0
    }
    
    return results


# ========================================================
# ä¸»è¦åˆ†æå‡½æ•¸
# ========================================================

def calculate_reprojection_errors(
    data_3d: list,
    data_2d_side: list,
    data_2d_45: list,
    P1: np.ndarray,
    P2: np.ndarray,
    config: ValidationConfig,
    distortion_side: Optional[np.ndarray] = None,
    distortion_45: Optional[np.ndarray] = None
) -> dict:
    """è¨ˆç®—é‡æŠ•å½±èª¤å·®ä¸¦æ”¶é›†çµ±è¨ˆè³‡è¨Šã€‚"""

    num_frames = min(len(data_3d), len(data_2d_side), len(data_2d_45))
    keypoints = KEYPOINT_NAMES_EN

    errors_side = {kp: [] for kp in keypoints}
    errors_45 = {kp: [] for kp in keypoints}

    per_frame_error_side = []
    per_frame_error_45 = []
    Z_values = []
    error_records = []

    distortion_side_arr = (
        np.array(distortion_side, dtype=float)
        if distortion_side is not None and len(distortion_side)
        else None
    )
    distortion_45_arr = (
        np.array(distortion_45, dtype=float)
        if distortion_45 is not None and len(distortion_45)
        else None
    )

    for frame_idx in range(num_frames):
        f3d = data_3d[frame_idx]
        f2d_side = data_2d_side[frame_idx]
        f2d_45 = data_2d_45[frame_idx]

        frame_err_side = []
        frame_err_45 = []

        for kp in keypoints:
            point_3d = get_keypoint_safely(f3d, kp)
            if point_3d is None:
                continue

            # ä¿®æ­£ï¼šå› ç‚ºè¼¸å…¥çš„ 3D æª”æ¡ˆ Y, Z è»¸å·²è¢«åè½‰ï¼Œéœ€è½‰å›åŸå§‹åº§æ¨™ç³»ä»¥é…åˆ P çŸ©é™£
            X_h = np.array([point_3d[0], -point_3d[1], point_3d[2], 1.0], dtype=float)

            if kp == "nose":
                Z_values.append(point_3d[2])

            point_2d_side = get_keypoint_2d(f2d_side, kp)
            if point_2d_side is not None:
                obs = point_2d_side[:2]
                proj = project_point(P1, X_h, distortion_side_arr)
                if not np.any(np.isnan(proj)):
                    err = float(np.linalg.norm(obs - proj))
                    errors_side[kp].append(err)
                    frame_err_side.append(err)
                    error_records.append(("side", frame_idx, kp, err, point_3d[2]))

            point_2d_45 = get_keypoint_2d(f2d_45, kp)
            if point_2d_45 is not None:
                obs = point_2d_45[:2]
                proj = project_point(P2, X_h, distortion_45_arr)
                if not np.any(np.isnan(proj)):
                    err = float(np.linalg.norm(obs - proj))
                    errors_45[kp].append(err)
                    frame_err_45.append(err)
                    error_records.append(("45", frame_idx, kp, err, point_3d[2]))

        per_frame_error_side.append(np.mean(frame_err_side) if frame_err_side else np.nan)
        per_frame_error_45.append(np.mean(frame_err_45) if frame_err_45 else np.nan)

    return {
        'errors_side': errors_side,
        'errors_45': errors_45,
        'per_frame_error_side': np.array(per_frame_error_side, dtype=float),
        'per_frame_error_45': np.array(per_frame_error_45, dtype=float),
        'Z_values': np.array(Z_values, dtype=float),
        'error_records': error_records,
        'num_frames': num_frames,
        'keypoints': keypoints
    }


def analyze_reprojection_results(error_data: dict, config: ValidationConfig) -> dict:
    """
    åˆ†æé‡æŠ•å½±èª¤å·®çµæœ
    
    åƒæ•¸:
        error_data: èª¤å·®æ•¸æ“š
        config: é©—è­‰é…ç½®
    
    è¿”å›:
        dict: åˆ†æçµæœ
    """
    errors_side = error_data['errors_side']
    errors_45 = error_data['errors_45']
    keypoints = error_data['keypoints']
    error_records = error_data['error_records']
    
    global_stat = {}
    all_side = []
    all_45 = []

    # è¨ˆç®—å„é—œç¯€é»çµ±è¨ˆ
    for kp in keypoints:
        s = np.array(errors_side[kp])
        d = np.array(errors_45[kp])

        mean_s = float(s.mean()) if s.size else 0
        std_s = float(s.std()) if s.size else 0
        mean_d = float(d.mean()) if d.size else 0
        std_d = float(d.std()) if d.size else 0

        if s.size:
            all_side.extend(s)
        if d.size:
            all_45.extend(d)

        global_stat[kp] = {
            "side_mean": mean_s,
            "side_std": std_s,
            "45_mean": mean_d,
            "45_std": std_d,
            "avg_error": (mean_s + mean_d) / 2,
        }

    all_side = np.array(all_side, dtype=float)
    all_45 = np.array(all_45, dtype=float)

    # å…¨åŸŸçµ±è¨ˆ
    global_mean_side = float(all_side.mean()) if all_side.size else 0.0
    global_mean_45 = float(all_45.mean()) if all_45.size else 0.0
    
    # ç•°å¸¸å€¼æª¢æ¸¬
    out_s = int(np.sum(all_side > config.reprojection_outlier_threshold))
    out_45 = int(np.sum(all_45 > config.reprojection_outlier_threshold))
    
    # TOP 10 æœ€å¤§èª¤å·®
    error_sorted = sorted(error_records, key=lambda r: r[3], reverse=True)
    
    # å“è³ªè©•ä¼°
    quality_level = config.get_quality_level_reprojection(
        (global_mean_side + global_mean_45) / 2
    )
    
    return {
        'global_stat': global_stat,
        'all_side': all_side,
        'all_45': all_45,
        'global_mean_side': global_mean_side,
        'global_mean_45': global_mean_45,
        'outlier_count_side': out_s,
        'outlier_count_45': out_45,
        'error_sorted': error_sorted,
        'quality_level': quality_level
    }


def print_analysis_report(
    error_data: dict,
    analysis: dict,
    config: ValidationConfig,
    depth_relationship: Optional[dict] = None,
    temporal_stability: Optional[dict] = None
) -> None:
    """
    åˆ—å°åˆ†æå ±å‘Š
    
    åƒæ•¸:
        error_data: èª¤å·®æ•¸æ“š
        analysis: åˆ†æçµæœ
        config: é©—è­‰é…ç½®
    """
    print("\n" + "=" * 100)
    print("ã€1. é‡æŠ•å½±èª¤å·®çµ±è¨ˆ - å„é—œç¯€é»åˆ†æã€‘")
    print("=" * 100)
    
    for kp in error_data['keypoints']:
        stat = analysis['global_stat'][kp]
        print(f"{kp:<15s} Side={stat['side_mean']:6.2f}Â±{stat['side_std']:5.2f}   "
              f"45Â°={stat['45_mean']:6.2f}Â±{stat['45_std']:5.2f}")
    
    print("\n" + "=" * 100)
    print("ã€2. å…¨åŸŸèª¤å·®ç¸½è¦½ã€‘")
    print("=" * 100)
    print(f"Side ç›¸æ©Ÿå¹³å‡èª¤å·®: {analysis['global_mean_side']:.2f} px")
    print(f"45Â° ç›¸æ©Ÿå¹³å‡èª¤å·®:  {analysis['global_mean_45']:.2f} px")
    print(f"æ•´é«”å¹³å‡èª¤å·®:      {(analysis['global_mean_side'] + analysis['global_mean_45'])/2:.2f} px")
    print(f"å“è³ªç­‰ç´š:          {analysis['quality_level']}")
    
    all_side = analysis['all_side']
    all_45 = analysis['all_45']
    
    print("\n" + "=" * 100)
    print("ã€3. èª¤å·®åˆ†ä½ˆçµ±è¨ˆï¼ˆä¸­ä½æ•¸/æœ€å¤§å€¼/ç™¾åˆ†ä½ï¼‰ã€‘")
    print("=" * 100)
    if all_side.size:
        print(f"Side median={np.median(all_side):.2f}, max={np.max(all_side):.2f}, "
              f"95th={np.percentile(all_side, 95):.2f}")
    else:
        print("Side ç„¡æœ‰æ•ˆèª¤å·®æ¨£æœ¬")
    if all_45.size:
        print(f"45Â° median={np.median(all_45):.2f}, max={np.max(all_45):.2f}, "
              f"95th={np.percentile(all_45, 95):.2f}")
    else:
        print("45Â° ç„¡æœ‰æ•ˆèª¤å·®æ¨£æœ¬")
    
    print("\n" + "=" * 100)
    print(f"ã€4. ç•°å¸¸å€¼æª¢æ¸¬ï¼ˆèª¤å·® > {config.reprojection_outlier_threshold} pxï¼‰ã€‘")
    print("=" * 100)
    side_outlier_rate = safe_ratio(analysis['outlier_count_side'], len(all_side))
    cam45_outlier_rate = safe_ratio(analysis['outlier_count_45'], len(all_45))
    print(f"Side ç›¸æ©Ÿç•°å¸¸å€¼: {analysis['outlier_count_side']} å€‹ ({side_outlier_rate:.2f}%)")
    print(f"45Â° ç›¸æ©Ÿç•°å¸¸å€¼:  {analysis['outlier_count_45']} å€‹ ({cam45_outlier_rate:.2f}%)")
    
    print("\n" + "=" * 100)
    print(f"ã€5. ç•°å¸¸èª¤å·®è©³ç´°åˆ†æã€‘ï¼ˆèª¤å·® > {config.reprojection_outlier_threshold:.1f} pxï¼‰")
    print("=" * 100)
    
    if 'detailed_anomalies' in analysis and analysis['detailed_anomalies'].get('total_count', 0) > 0:
        detail = analysis['detailed_anomalies']
        total = detail['total_count']
        severe = detail['by_severity']['severe']
        moderate = detail['by_severity']['moderate']
        mild = detail['by_severity']['mild']
        
        print(f"ç¸½ç•°å¸¸æ•¸: {total} å€‹ï¼ˆSide: {analysis['outlier_count_side']}, 45Â°: {analysis['outlier_count_45']}ï¼‰")
        print()
        
        # åš´é‡ç¨‹åº¦åˆ†ç´š
        print("â–¸ æŒ‰åš´é‡ç¨‹åº¦åˆ†ç´š:")
        severe_threshold = config.reprojection_outlier_threshold * 2
        moderate_threshold = config.reprojection_outlier_threshold * 1.5
        print(f"  â€¢ åš´é‡ (>{severe_threshold:.0f}px):  {len(severe):3d} å€‹ ({len(severe)/total*100:5.1f}%) - [!] å»ºè­°å„ªå…ˆæª¢æŸ¥")
        print(f"  â€¢ ä¸­ç­‰ ({moderate_threshold:.0f}-{severe_threshold:.0f}px): {len(moderate):3d} å€‹ ({len(moderate)/total*100:5.1f}%)")
        print(f"  â€¢ è¼•å¾® ({config.reprojection_outlier_threshold:.0f}-{moderate_threshold:.0f}px): {len(mild):3d} å€‹ ({len(mild)/total*100:5.1f}%)")
        print()
        
        # æŒ‰é—œç¯€é»çµ±è¨ˆ TOP 5
        kp_stats = detail['by_keypoint']
        sorted_kps = sorted(kp_stats.items(), key=lambda x: x[1]['count'], reverse=True)[:5]
        if sorted_kps:
            print("â–¸ æŒ‰é—œç¯€é»çµ±è¨ˆï¼ˆTOP 5ï¼‰:")
            for idx, (kp, stats) in enumerate(sorted_kps, 1):
                side_pct = stats['side_ratio'] * 100
                cam_info = f"Side ç›¸æ©Ÿä½” {side_pct:.0f}%" if side_pct > 60 else f"45Â° ç›¸æ©Ÿä½” {100-side_pct:.0f}%" if side_pct < 40 else "å…©ç›¸æ©Ÿåˆ†å¸ƒå‡å‹»"
                print(f"  {idx}. {kp:<15s}: {stats['count']:3d} å€‹ ({stats['count']/total*100:5.1f}%) - {cam_info}")
            print()
        
        # é€£çºŒç•°å¸¸å€æ®µ
        segments = detail['continuous_segments']
        if segments:
            print("â–¸ é€£çºŒç•°å¸¸å€æ®µï¼ˆâ‰¥5 å¹€ï¼‰:")
            for seg in segments[:5]:  # æœ€å¤šé¡¯ç¤º 5 å€‹
                cam_label = "Side" if seg['camera'] == "side" else "45Â°"
                print(f"  â€¢ Frame {seg['start_frame']:3d}-{seg['end_frame']:3d} ({seg['duration']:2d} å¹€): "
                      f"{seg['keypoint']:<12s} @ {cam_label:<4s}, å¹³å‡ {seg['mean_error']:5.1f}px, "
                      f"æœ€å¤§ {seg['max_error']:5.1f}px")
            print()
        
        # åš´é‡ç•°å¸¸ - å…¨éƒ¨é¡¯ç¤º
        if severe:
            print("â”€" * 100)
            print(f"[!] åš´é‡ç•°å¸¸ (>{severe_threshold:.0f}px) - å…¨éƒ¨ {len(severe)} å€‹:")
            print("â”€" * 100)
            for idx, (cam, frame, kp, err, z) in enumerate(severe, 1):
                cam_label = "Side" if cam == "side" else "45Â° "
                print(f" {idx:3d}. Frame {frame:3d}: {kp:<15s} @ {cam_label} - {err:6.2f}px (z={z:7.1f}mm)")
            print()
        
        # ä¸­ç­‰ç•°å¸¸ - é¡¯ç¤ºå‰ 10 å€‹
        if moderate:
            print("â”€" * 100)
            display_count = min(10, len(moderate))
            print(f"ğŸ“Š ä¸­ç­‰ç•°å¸¸ ({moderate_threshold:.0f}-{severe_threshold:.0f}px) - é¡¯ç¤ºå‰ {display_count} å€‹ï¼Œå…± {len(moderate)} å€‹:")
            print("â”€" * 100)
            moderate_sorted = sorted(moderate, key=lambda x: x[3], reverse=True)
            for idx, (cam, frame, kp, err, z) in enumerate(moderate_sorted[:display_count], 1):
                cam_label = "Side" if cam == "side" else "45Â° "
                print(f" {idx:3d}. Frame {frame:3d}: {kp:<15s} @ {cam_label} - {err:6.2f}px (z={z:7.1f}mm)")
            if len(moderate) > display_count:
                print(f"\n â‹® å…¶é¤˜ {len(moderate) - display_count} å€‹ä¸­ç­‰ç•°å¸¸è«‹åƒé–± JSON è¼¸å‡º")
            print()
        
        # è¼•å¾®ç•°å¸¸ - åªé¡¯ç¤ºçµ±è¨ˆ
        if mild:
            print("â”€" * 100)
            print(f"ğŸ“‹ è¼•å¾®ç•°å¸¸ ({config.reprojection_outlier_threshold:.0f}-{moderate_threshold:.0f}px) - å…± {len(mild)} å€‹ï¼Œè©³è¦‹ JSON")
            print("â”€" * 100)
            print()
        
        print("ğŸ’¾ å®Œæ•´ç•°å¸¸åˆ—è¡¨å·²å„²å­˜è‡³ JSON:")
        print("   âœ“ detailed_anomalies.all_anomalies     - æ‰€æœ‰ç•°å¸¸ï¼ˆæŒ‰èª¤å·®æ’åºï¼‰")
        print("   âœ“ detailed_anomalies.by_keypoint       - æŒ‰é—œç¯€é»åˆ†çµ„")
        print("   âœ“ detailed_anomalies.continuous_segments - é€£çºŒç•°å¸¸å€æ®µ")
    else:
        print("[OK] æœªæª¢æ¸¬åˆ°ç•°å¸¸èª¤å·®")
    
    # ç›¸æ©Ÿåå·®åˆ†æ
    diff = analysis['global_mean_side'] - analysis['global_mean_45']
    print("\n" + "=" * 100)
    print("ã€6. ç›¸æ©Ÿåå·®åˆ†æï¼ˆSide vs 45Â°ï¼‰ã€‘")
    print("=" * 100)
    print(f"Side - 45Â° èª¤å·®å·®ç•°: {diff:.2f} px")
    if diff > 0:
        print("[OK] çµè«–: Side ç›¸æ©Ÿèª¤å·®è¼ƒå¤§")
    else:
        print("[OK] çµè«–: 45Â° ç›¸æ©Ÿèª¤å·®è¼ƒå¤§")

    if depth_relationship:
        print("\n" + "=" * 100)
        print("ã€7. èª¤å·® vs æ·±åº¦ é—œä¿‚ã€‘")
        print("=" * 100)
        print(f"ç›¸é—œä¿‚æ•¸: {depth_relationship['corr']:.3f}")
        print(f"è¶¨å‹¢: error = {depth_relationship['slope']:.4f} * depth + {depth_relationship['intercept']:.2f}")
        print(f"æ·±åº¦ç¯„åœ: {depth_relationship['depth_min']:.1f} ~ {depth_relationship['depth_max']:.1f} mm")

    if temporal_stability:
        print("\n" + "=" * 100)
        print("ã€8. èª¤å·®æ™‚é–“ç©©å®šæ€§ã€‘")
        print("=" * 100)
        label_map = {"side": "Side", "camera_45": "45Â°"}
        for cam_label, stats in temporal_stability.items():
            pretty_label = label_map.get(cam_label, cam_label)
            if stats.get("samples", 0) == 0:
                print(f"{pretty_label}: ç„¡æœ‰æ•ˆæ¨£æœ¬")
                continue
            print(f"{pretty_label}: å¹³å‡ {stats['mean']:.2f}px, CV={stats['cv']:.2f}%, çªæ³¢ {stats['spike_count']} æ¬¡, æœ€å¤§è·³è®Š {stats['max_jump']:.2f}px")


def validate_reprojection_analysis(
    json_3d_path: str,
    json_2d_side_path: str,
    json_2d_45_path: str,
    P1: np.ndarray,
    P2: np.ndarray,
    output_json_path: str = None,
    config_path: str = None
) -> dict:
    """
    é‡æŠ•å½±èª¤å·®é©—è­‰åˆ†æï¼ˆä¸»å‡½æ•¸ï¼‰
    
    åƒæ•¸:
        json_3d_path: 3D è»Œè·¡ JSON æª”æ¡ˆè·¯å¾‘
        json_2d_side_path: Side ç›¸æ©Ÿ 2D è»Œè·¡ JSON æª”æ¡ˆè·¯å¾‘
        json_2d_45_path: 45Â° ç›¸æ©Ÿ 2D è»Œè·¡ JSON æª”æ¡ˆè·¯å¾‘
        P1: Side ç›¸æ©ŸæŠ•å½±çŸ©é™£ (3x4)
        P2: 45Â° ç›¸æ©ŸæŠ•å½±çŸ©é™£ (3x4)
        output_json_path: è¼¸å‡ºçµæœ JSON è·¯å¾‘ï¼ˆå¯é¸ï¼‰
        config_path: é…ç½®æª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰
    
    è¿”å›:
        dict: å®Œæ•´åˆ†æçµæœ
    
    ç¯„ä¾‹:
        >>> P1 = np.array([[fx, 0, cx, 0], [0, fy, cy, 0], [0, 0, 1, 0]])
        >>> P2 = np.array([[...], [...], [...]])
        >>> results = validate_reprojection_analysis(
        ...     '3d_data.json', '2d_side.json', '2d_45.json', P1, P2
        ... )
    """
    # è¼‰å…¥é…ç½®
    config = load_config(config_path)
    
    # è¼‰å…¥æ•¸æ“š
    print(f"\nè¼‰å…¥æ•¸æ“š...")
    data_3d = load_json_file(json_3d_path)
    data_2d_side = load_json_file(json_2d_side_path)
    data_2d_45 = load_json_file(json_2d_45_path)
    
    print(f"3D è»Œè·¡: {len(data_3d)} å¹€")
    print(f"2D Side: {len(data_2d_side)} å¹€")
    print(f"2D 45Â°:  {len(data_2d_45)} å¹€")
    
    # é©—è­‰ç›¸æ©Ÿå…§åƒ
    print(f"\né©—è­‰ç›¸æ©Ÿå…§åƒ...")
    intrinsics_side = validate_camera_intrinsics(P1, "Side Camera")
    intrinsics_45 = validate_camera_intrinsics(P2, "45Â° Camera")
    
    if not intrinsics_side["is_valid"]:
        print(f"[!] Side ç›¸æ©Ÿå…§åƒé©—è­‰å¤±æ•—:")
        for err in intrinsics_side["errors"]:
            print(f"  - {err}")
    
    if not intrinsics_45["is_valid"]:
        print(f"[!] 45Â° ç›¸æ©Ÿå…§åƒé©—è­‰å¤±æ•—:")
        for err in intrinsics_45["errors"]:
            print(f"  - {err}")
    
    # è¨ˆç®—é‡æŠ•å½±èª¤å·®
    print(f"\nè¨ˆç®—é‡æŠ•å½±èª¤å·®...")
    error_data = calculate_reprojection_errors(
        data_3d,
        data_2d_side,
        data_2d_45,
        P1,
        P2,
        config,
        distortion_side=config.side_camera_distortion,
        distortion_45=config.camera_45_distortion,
    )
    
    # åˆ†æçµæœ
    print(f"\nåˆ†æèª¤å·®æ•¸æ“š...")
    analysis = analyze_reprojection_results(error_data, config)
    depth_relationship = analyze_error_depth_relationship(error_data['error_records'])
    temporal_stability = analyze_temporal_stability(
        error_data['per_frame_error_side'],
        error_data['per_frame_error_45'],
        config
    )
    
    # è©³ç´°ç•°å¸¸åˆ†æ
    print(f"\nåŸ·è¡Œè©³ç´°ç•°å¸¸åˆ†æ...")
    detailed_anomalies = analyze_detailed_anomalies(error_data['error_records'], config)
    analysis['detailed_anomalies'] = detailed_anomalies

    # æ•´ç†å®Œæ•´èª¤å·®æ•¸æ“šä¾›å‰ç«¯ç¹ªåœ–
    full_keypoint_details = {kp: [] for kp in error_data['keypoints']}
    for cam, frame, kp, err, z in error_data['error_records']:
        full_keypoint_details[kp].append({
            "frame": int(frame),
            "camera": cam,
            "error": float(err),
            "depth_z": float(z)
        })
    
    # åˆ—å°å ±å‘Š
    print_analysis_report(
        error_data,
        analysis,
        config,
        depth_relationship=depth_relationship,
        temporal_stability=temporal_stability
    )
    
    # æ•´åˆçµæœï¼ˆç¢ºä¿æ‰€æœ‰æ•¸å€¼éƒ½ç¶“éåºåˆ—åŒ–è™•ç†ï¼‰
    results = {
        "metadata": {
            "analysis_time": datetime.now().isoformat(),
            "source_file": str(json_3d_path),
            "total_frames": int(error_data['num_frames']),
            "total_keypoints": int(len(error_data['keypoints'])),
            "analysis_type": "Reprojection Error Analysis"
        },
        "camera_intrinsics_validation": {
            "side_camera": intrinsics_side,
            "45_camera": intrinsics_45
        },
        "global_stats": {
            "overall_mean": (analysis['global_mean_side'] + analysis['global_mean_45']) / 2,
            "side_mean": analysis['global_mean_side'],
            "45_mean": analysis['global_mean_45'],
            "side_median": float(np.median(analysis['all_side'])) if analysis['all_side'].size else 0.0,
            "45_median": float(np.median(analysis['all_45'])) if analysis['all_45'].size else 0.0,
            "side_max": float(np.max(analysis['all_side'])) if analysis['all_side'].size else 0.0,
            "45_max": float(np.max(analysis['all_45'])) if analysis['all_45'].size else 0.0,
            "side_95th": float(np.percentile(analysis['all_side'], 95)) if analysis['all_side'].size else 0.0,
            "45_95th": float(np.percentile(analysis['all_45'], 95)) if analysis['all_45'].size else 0.0,
            "outlier_count_side": analysis['outlier_count_side'],
            "outlier_count_45": analysis['outlier_count_45'],
            "outlier_rate_side": safe_ratio(analysis['outlier_count_side'], len(analysis['all_side'])),
            "outlier_rate_45": safe_ratio(analysis['outlier_count_45'], len(analysis['all_45'])),
            "camera_bias": analysis['global_mean_side'] - analysis['global_mean_45'],
            "quality_level": analysis['quality_level']
        },
        "keypoint_errors": [
            {
                "name": kp,
                **analysis['global_stat'][kp]
            }
            for kp in error_data['keypoints']
        ],
        "per_frame_errors": {
            "frames": list(range(error_data['num_frames'])),
            "side": [None if np.isnan(v) else float(v) for v in error_data['per_frame_error_side']],
            "45": [None if np.isnan(v) else float(v) for v in error_data['per_frame_error_45']]
        },
        "top10_worst_errors": [
            {
                "rank": i + 1,
                "camera": cam,
                "frame": int(frame),
                "keypoint": kp,
                "error": float(err),
                "depth_z": float(z)
            }
            for i, (cam, frame, kp, err, z) in enumerate(analysis['error_sorted'][:10])
        ],
        "error_depth_relationship": depth_relationship,
        "temporal_stability": temporal_stability,
        "full_keypoint_details": full_keypoint_details,
        "detailed_anomalies": {
            "summary": {
                "total_count": detailed_anomalies.get('total_count', 0),
                "severe_count": len(detailed_anomalies.get('by_severity', {}).get('severe', [])),
                "moderate_count": len(detailed_anomalies.get('by_severity', {}).get('moderate', [])),
                "mild_count": len(detailed_anomalies.get('by_severity', {}).get('mild', []))
            },
            "by_keypoint": detailed_anomalies.get('by_keypoint', {}),
            "continuous_segments": detailed_anomalies.get('continuous_segments', []),
            "all_anomalies": [
                {
                    "frame": int(frame),
                    "camera": cam,
                    "keypoint": kp,
                    "error": float(err),
                    "depth_z": float(z),
                    "severity": "severe" if err > config.reprojection_outlier_threshold * 2 else 
                               "moderate" if err > config.reprojection_outlier_threshold * 1.5 else "mild"
                }
                for cam, frame, kp, err, z in (detailed_anomalies.get('by_severity', {}).get('severe', []) +
                                               detailed_anomalies.get('by_severity', {}).get('moderate', []) +
                                               detailed_anomalies.get('by_severity', {}).get('mild', []))
            ]
        } if detailed_anomalies.get('total_count', 0) > 0 else {}
    }
    
    # ä¿å­˜çµæœ
    if output_json_path is None:
        output_json_path = generate_output_path(
            json_3d_path, '_step1_reprojection_error_results'
        )
    
    save_json_results(results, output_json_path)
    print(f"\n[OK] çµæœå·²å„²å­˜è‡³: {output_json_path}")
    
    return results


# ========================================================
# ä¸»ç¨‹å¼
# ========================================================

if __name__ == "__main__":
    # é è¨­æŠ•å½±çŸ©é™£ï¼ˆè«‹ä¾å¯¦éš›æ¨™å®šçµæœä¿®æ”¹ï¼‰
    P1 = np.array([
        [917.153880, 0.000000, 994.529968, 0.000000],
        [0.000000, 920.803487, 531.057076, 0.000000],
        [0.000000, 0.000000, 1.000000, 0.000000],
    ])

    P2 = np.array([
        [286.476533, 43.805594, 1301.943509, -765436.820164],
        [-309.560886, 957.641377, 401.534167, 365723.173062],
        [-0.553187, 0.008475, 0.833014, 660.964347],
    ])

    # å‘½ä»¤åˆ—åƒæ•¸æ”¯æ´
    if len(sys.argv) >= 4:
        json_3d_path = sys.argv[1]
        json_2d_side_path = sys.argv[2]
        json_2d_45_path = sys.argv[3]
        output_json_path = sys.argv[4] if len(sys.argv) > 4 else None
        config_path = sys.argv[5] if len(sys.argv) > 5 else None
    else:
        # é è¨­æ¸¬è©¦è·¯å¾‘
        json_3d_path = "trajectory__2/0306_3__2(3D_trajectory_smoothed).json"
        json_2d_side_path = "trajectory__2/0306_3__2_side(2D_trajectory_smoothed).json"
        json_2d_45_path = "trajectory__2/0306_3__2_45(2D_trajectory_smoothed).json"
        output_json_path = None
        config_path = None
        print("æç¤º: å¯ä½¿ç”¨å‘½ä»¤åˆ—åƒæ•¸:")
        print("  python step1_Reprojection_Error.py <3d_json> <2d_side_json> <2d_45_json> [output_json] [config_json]")

    # åŸ·è¡Œåˆ†æ
    try:
        results = validate_reprojection_analysis(
            json_3d_path,
            json_2d_side_path,
            json_2d_45_path,
            P1, P2,
            output_json_path,
            config_path
        )
    except Exception as e:
        print(f"\n[ERROR] åˆ†æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
