"""
GPU è¨˜æ†¶é«”æª¢æŸ¥å’Œæ¸…ç†å·¥å…·
"""
import torch
import gc

def check_gpu_status():
    """æª¢æŸ¥ GPU ç‹€æ…‹"""
    print("ğŸ” æª¢æŸ¥ GPU ç‹€æ…‹...")
    
    if torch.cuda.is_available():
        print(f"âœ… CUDA å¯ç”¨")
        print(f"ğŸ“± GPU æ•¸é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            total_memory = props.total_memory / 1024**3  # GB
            allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
            cached = torch.cuda.memory_reserved(i) / 1024**3  # GB
            
            print(f"ğŸ® GPU {i}: {props.name}")
            print(f"   ç¸½è¨˜æ†¶é«”: {total_memory:.2f} GB")
            print(f"   å·²åˆ†é…: {allocated:.2f} GB")
            print(f"   å·²ç·©å­˜: {cached:.2f} GB")
            print(f"   å¯ç”¨: {total_memory - cached:.2f} GB")
    else:
        print("âŒ CUDA ä¸å¯ç”¨")

def clear_all_gpu_memory():
    """æ¸…ç†æ‰€æœ‰ GPU è¨˜æ†¶é«”"""
    print("\nğŸ§¹ æ¸…ç† GPU è¨˜æ†¶é«”...")
    
    if torch.cuda.is_available():
        # æ¸…ç†æ‰€æœ‰ GPU
        for i in range(torch.cuda.device_count()):
            with torch.cuda.device(i):
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
        
        # å¼·åˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        print("âœ… GPU è¨˜æ†¶é«”æ¸…ç†å®Œæˆ")
    else:
        print("âš ï¸ ç„¡ GPU å¯æ¸…ç†")

if __name__ == "__main__":
    check_gpu_status()
    clear_all_gpu_memory()
    print("\n" + "="*50)
    print("æ¸…ç†å¾Œç‹€æ…‹:")
    check_gpu_status()